---
layout: post
title: Portfolio
--- 

<h1>Projects:</h1>

<ul><h2>Answering user queries with ChatGpt for US Wealth Management Client</h2>
<li>Built large language model (LLM, GPT 4) based solution to answer customer inquiries related to policies, claims, etc.
</li>
<li>Design prompt templates for BI users, enhancing the output of large language models. 
</li>
<li>Implemented Retrieval-augmented generation (RAG) based SQL Agent from langchain library to generate sql queries.
</li>
<li>Architected and led the development and deployment of end-to-end solutions on Azure, utilizing components such as Microsoft OpenAI, Docker, Azure DevOps, Web Apps, ACR, and ACI. 
</li>
<li>Reduced day-to-day dependency of users on the backend team by 65% and decreased the number of support tickets for data requests by 60%.
</li>
<li>Used terraform as IaC to deploy cloud components and services on Azure.
</li></ul>

<ul><h2>Customer Lifetime Value for major German Automobile Client</h2>
<li>Created models for ML model to derive CLTV of customers to find target customers for promotional ads.
</li>
<li>Performed data cleaning and manipulation using PySpark on Azure Databricks.
</li>
<li>Implemented CI/CD pipeline with Azure Devops to trigger databricks job
</li>
<li>Used Mlflow to track training experiment data and manage model deployment lifecycle.
</li></ul>

<ul><h2>Automate Answering Due Diligence Questionnaire for US Wealth Management Client</h2>
<li>Implemented parsing logic to extract components like text and tables from docx word templates, trained ML models to classify text into questions, headers, and others, and developed a similarity model using BERT to match extracted questions with the existing corpus and extract answers. 
</li>
<li>Deployed the end-to-end model using Sagemaker notebooks on AWS Step Function and utilized schema-less NOSQL DynamoDB for efficient data storage and retrieval. 
</li>
<li>Achieved a reduction in turnaround time for submitting completed due diligence questionnaires from days to minutes, resulting in yearly time-effort savings of $500k.
</li></ul>

<ul><h2>Bias Detection and Mitigation in Loan Application for UK Banking Client</h2>
<li>Researched and experimented with statistical techniques and frameworks to detect and mitigate bias in machine learning models for loan applications. 
</li>
<li>Used AIF-360 library to perform constraint optimization in training TensorFlow models and the What-If tool to mitigate bias post-training. 
</li>
<li>Deployed the solution on Google Kubernetes Engine using Docker and Jenkins, resulting in fairer outcomes, reduced false negatives by 5%, and improved financial outcomes for nonprivileged groups by 10%.  
</li></ul>

<ul><h2>Connected Cars Platform IOT for Japanese Automobile Client</h2>
<li>Led development of a connected cars platform, utilizing AWS IoT Core to capture and analyze driving telemetric data.  
</li>
<li>Processed real-time data using AWS Kinesis Data Streams, enabling anomaly detection and generating alerts for end users.  
</li>
<li>Trained models using Xgboost and tracked performance using Mlflow, ensuring accurate predictions and efficient monitoring.  
</li>
<li>Developed data pipelines using Airflow DAG to detect anomalies in driving behaviors and generate alerts for neighboring vehicles, reducing the frequency of accidents by 40%. 
</li>
<li>Built and deployed microservices on AWS EKS using Jenkins and deployed cloud components using AWS CloudFormation templates. 
</li></ul>

<ul><h2>Image Classification and Processing - Background removal in Images for German Bank </h2>
<li>Implemented automatic detection and removal of background from photographs submitted for identity cards, improving customer engagement and satisfaction by 20%.  
</li>
<li>Utilized OpenCV for image preprocessing and trained deep learning models PIX to PIX GAN using TensorFlow to detect and remove backgrounds from photographs. 
</li></ul>

<ul><h2>Automate Categorization and File Ingestion for UK Reinsurer</h2>
<li>Led the development and implementation of an intelligent file ingestion project, utilizing NLP techniques and the spacy library in Python to identify and preprocess Bordeau files. 
</li>
<li>Trained and deployed a model using Azure ML Studio, automating the classification and tagging of files, resulting in a 75% reduction in manual effort. 
</li>
<li>Implemented the "Schema/Data Drift" component to trigger model retraining and redeployment when required conditions were met, ensuring continuous accuracy and efficiency. 
</li>
<li>Built an end-to-end execution pipeline using Azure Data Factory, enabling real-time generation of reports and statistics, reducing access time from days to seconds. 
</li></ul>
 
<ul><h2>Loan Default Prediction for UK Banking Client</h2>
<li>Performed data exploration using Pandas, NumPy, and Tableau and preprocessed data using Python and Pandas 
</li>
<li>Used sklearn for model training, tuning using hyperparameter tuning, and k-fold cross-validation techniques for model validation. 
</li>
<li>Created a dashboard for presenting insights using Tableau. 
</li>
<li>Overall, a 20% reduction was achieved in false positives and the system became more adaptive to incorporate new patterns.
</li></ul>
